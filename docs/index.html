<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>High-resolution image reconstruction with latent diffusion models from human brain activity</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px;
      line-height: 1.6;
    }
    h1 {
      font-size: 1.8em;
      margin-bottom: 80px;
      color:#034587;
      text-align: center;
    }
    h2 {
      font-size: 1.4em;
      margin-bottom: 1em;
      color:#034587;
      text-align: center;
    }
    .subtitle {
      font-size: 1.6em;
      margin-bottom: 1em;
      text-align: center;
    }
    .highlight {
        color: #e30a0a; /* ACR 2025の文字だけ赤にする */
      }
    .authors, .affiliations {
      margin: 0;
      padding: 0;
      text-align: center;
    }
    .links {
        text-align: center;
      }      
    .links a {
      margin-right: 1em;
      text-decoration: none;
      color: #0066cc;
    }
    .links a:hover {
      text-decoration: underline;
    }
    hr {
      margin: 2em 0;
    }
  </style>
</head>
<body>

  <h1>Triple Phase Transitions: Understanding the Learning Dynamics of Large Language Models from a Neuroscience Perspective</h1>
  <p class="subtitle">Accepted at <span class="highlight">~~~ 2025</span></p>

  <p class="authors">
    <strong>
      Yuko Nakagi<sup>1,2†</sup>,
      Keigo Tada<sup>1,2*</sup>,
      Sota Yoshino<sup>1,2*</sup>,
      Shinji Nishimoto<sup>1,2‡</sup>,
      Yu Takagi<sup>1,2,3‡</sup>
    </strong>
  </p>
  <p class="affiliations">
    1 The University of Osaka, Japan<br>
    2 National Institute of Information and Communications Technology, Japan<br>
    3 National Institute of Informatics, Japan<br>
    <sup>*</sup>Equal first author<br>
    <sup>†</sup>Team lead<br>
    <sup>‡</sup>Equal last author<br>
  </p>
  

  <p class="links">
    <a href="#" target="_blank">Paper</a>
    <a href="#" target="_blank">Technical Paper</a>
    <a href="#" target="_blank">Code</a>
    <a href="#" target="_blank">FAQ (English)</a>
    <a href="#" target="_blank">FAQ (日本語)</a>
  </p>

  <hr>

  <h2>Abstract</h2>
  <p>
    Large language models (LLMs) often exhibitabrupt emergent behavior, whereby new abil-ities arise at certain points during their train-ing. This phenomenon, commonly referred toas a “phase transition”, remains poorly under-stood. In this study, we conduct an integrativeanalysis of such phase transitions by examin-ing three interconnected perspectives: the sim-ilarity between LLMs and the human brain, theinternal states of LLMs, and downstream taskperformance. We propose a novel interpreta-tion for the learning dynamics of LLMs thatvary in both training data and architecture, re-vealing that three phase transitions commonlyemerge across these models during training:(1) alignment with the entire brain surgesas LLMs begin adhering to task instructions(Brain Alignment and Instruction Following),(2) unexpectedly, LLMs diverge from the brainduring a period in which downstream task ac-curacy temporarily stagnates (Brain Detach-ment and Stagnation), and (3) alignment withthe brain reoccurs as LLMs become capable ofsolving the downstream tasks (Brain Realign-ment and Consolidation). These findings il-luminate the underlying mechanisms of phasetransitions in LLMs, while opening new av-enues for interdisciplinary research bridgingAI and neuroscience.
  </p>

  <p>
    
  </p>

  <p>
    
  </p>

</body>
</html>
